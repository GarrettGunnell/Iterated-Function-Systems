#pragma kernel SingleThreadedScan
#pragma kernel GlobalReduce
#pragma kernel FinalReduce
#pragma kernel ReductionToTransformation

#pragma kernel FirstLODIteration
#pragma kernel LODIteration

RWStructuredBuffer<float3> _OutputBuffer, _InputBuffer;
uint _ReductionBufferSize;

[numthreads(1, 1, 1)]
void SingleThreadedScan(uint3 id : SV_DISPATCHTHREADID) {
    float3 minPos = 1000000000000000.0f;
    float3 maxPos = -1000000000000000.0f;

    for (uint i = 0; i < _ReductionBufferSize; ++i) {
        float3 v = _InputBuffer[i];

        minPos = min(minPos, v);
        maxPos = max(maxPos, v);
    }

    if (id.x == 0) {
        _OutputBuffer[0] = minPos;
        _OutputBuffer[1] = maxPos;
    }
}


// Reduction kernels and optimization progression referenced from https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf

#pragma multi_compile_local _ INTERLEAVED_ADDRESSING_DIVERGENT INTERLEAVED_ADDRESSING_BANK_CONFLICT SEQUENTIAL_ADDRESSING UNROLL_LAST_WARP
#pragma multi_compile_local _ DOUBLE_LOAD
#pragma multi_compile_local _ MIN_REDUCTION MAX_REDUCTION ADD_REDUCTION

#define REDUCTION_GROUP_SIZE 128


groupshared float3 gs_Reduce[REDUCTION_GROUP_SIZE];

float3 ReductionOperator(float3 v1, float3 v2) {
#ifdef MIN_REDUCTION
        return min(v1, v2);
#endif
#ifdef MAX_REDUCTION
        return max(v1, v2);
#endif
#ifdef ADD_REDUCTION
    return v1 + v2;
#endif

    return 0;
}

void Load(uint groupThreadID, uint globalThreadID, uint groupID, uint reductionBufferSize) {
#ifdef DOUBLE_LOAD // Reduction #4 -- Slide 18

    uint tid = groupThreadID;
    uint i = groupID * (reductionBufferSize * 2) + groupThreadID;

    gs_Reduce[tid] = ReductionOperator(_InputBuffer[i], _InputBuffer[i + reductionBufferSize]);

#else

    gs_Reduce[groupThreadID] = _InputBuffer[globalThreadID];

#endif

    AllMemoryBarrierWithGroupSync();

}

void Reduce(uint id, uint bufferSize) {
#ifdef INTERLEAVED_ADDRESSING_DIVERGENT // Reduction #1 -- Slide 9 (PROBLEM: Divergent branch in loop)
    
    [loop]
    for (uint s = 1; s < bufferSize; s *= 2) { 
        if (id % (2 * s) == 0) {
            float3 v = gs_Reduce[id];
            gs_Reduce[id] = ReductionOperator(v, gs_Reduce[id + s]);
        }

        GroupMemoryBarrierWithGroupSync();
    }

#endif
#ifdef INTERLEAVED_ADDRESSING_BANK_CONFLICT // Reduction #2 -- Slide 11 (PROBLEM: Different threads reference the same shared memory index)

    [loop]
    for (uint s = 1; s < bufferSize; s *= 2) {
        uint index = 2 * s * id;

        if (index < bufferSize) {
            float3 v = gs_Reduce[index];
            gs_Reduce[index] = ReductionOperator(v, gs_Reduce[index + s]);
        }

        GroupMemoryBarrierWithGroupSync();
    }

#endif
#ifdef SEQUENTIAL_ADDRESSING // Reduction #3 -- Slide 15

    [loop]
    for (uint s = bufferSize / 2; s > 0; s >>= 1) { // fyi s >>= 1 is equivalent to s /= 2
        if (id < s) {
            float3 v = gs_Reduce[id];
            gs_Reduce[id] = ReductionOperator(v, gs_Reduce[id + s]);
        }
        GroupMemoryBarrierWithGroupSync();
    }

#endif
#ifdef UNROLL_LAST_WARP // Reduction #5 -- Slide 22

    [unroll]
    for (uint s = bufferSize / 2; s > 0; s >>= 1) { // fyi s >>= 1 is equivalent to s /= 2
        if (id < s) {
            float3 v = gs_Reduce[id];
            gs_Reduce[id] = ReductionOperator(v, gs_Reduce[id + s]);
        }
        GroupMemoryBarrierWithGroupSync();
    }

#endif
}

[numthreads(REDUCTION_GROUP_SIZE, 1, 1)]
void GlobalReduce(uint3 id : SV_DISPATCHTHREADID, uint3 gtid : SV_GroupThreadID, uint3 gid : SV_GROUPID) {
    Load(gtid.x, id.x, gid.x, REDUCTION_GROUP_SIZE);

    Reduce(gtid.x, REDUCTION_GROUP_SIZE);

    if (gtid.x == 0) {
        _OutputBuffer[gid.x] = gs_Reduce[0];
    }
}

[numthreads(REDUCTION_GROUP_SIZE, 1, 1)]
void FinalReduce(uint3 id : SV_DISPATCHTHREADID, uint3 gtid : SV_GroupThreadID, uint3 gid : SV_GROUPID) {
    Load(id.x, id.x, gid.x, _ReductionBufferSize);

    Reduce(id.x, _ReductionBufferSize);

    if (id.x == 0) {
        #ifdef MIN_REDUCTION
        _OutputBuffer[0] = gs_Reduce[0];
        #endif
        #ifdef MAX_REDUCTION
        _OutputBuffer[1] = gs_Reduce[0];
        #endif
        #ifdef ADD_REDUCTION
        _OutputBuffer[2] = gs_Reduce[0];
        #endif
    }
}

RWStructuredBuffer<float4x4> _FinalTransformBuffer;
float _TargetBoundsSize, _ScalePadding, _ParticleCount;

[numthreads(1, 1, 1)]
void ReductionToTransformation(uint3 id : SV_DISPATCHTHREADID, uint3 gtid : SV_GroupThreadID, uint3 gid : SV_GROUPID) {
    float3 minPos = _InputBuffer[0];
    float3 maxPos = _InputBuffer[1];

    float3 midPos = _InputBuffer[2] / _ParticleCount;

    float x = abs(minPos.x - maxPos.x);
    float y = abs(minPos.y - maxPos.y);
    float z = abs(minPos.z - maxPos.z);

    float boundsExtent = max(distance(minPos, midPos), distance(maxPos, midPos));
    // float boundsExtent = max(x, max(y, z));

    float rescale = _TargetBoundsSize / boundsExtent;
    rescale *= _ScalePadding;

    midPos *= rescale;

    float4x4 finalTransform = {
        rescale, 0.0f, 0.0f, -midPos.x,
        0.0f, rescale, 0.0f, -midPos.y,
        0.0f, 0.0f, rescale, -midPos.z,
        0.0f, 0.0f, 0.0f, 1.0f
    };

    // float4x4 finalTransform = {
    //     1.0f, 0.0f, 0.0f, 0.0f,
    //     0.0f, 1.0f, 0.0f, 0.0f,
    //     0.0f, 0.0f, 1.0f, 0.0f,
    //     0.0f, 0.0f, 0.0f, 1.0f
    // };

    _FinalTransformBuffer[0] = finalTransform;
}

StructuredBuffer<float4x4> _Transformations;
int _TransformationCount;

[numthreads(1, 1, 1)]
void FirstLODIteration(uint3 id : SV_DISPATCHTHREADID) {
    for (int i = 0; i < _TransformationCount; ++i) {
        _OutputBuffer[id.x + i] = mul(_Transformations[i], float4(0, 0, 0, 1)).xyz;
    }
}

[numthreads(64, 1, 1)]
void LODIteration(uint3 id : SV_DISPATCHTHREADID) {
    float3 seed = _InputBuffer[id.x];

    int outputIndex = id.x * _TransformationCount;
    for (int i = 0; i < _TransformationCount; ++i) {
        _OutputBuffer[outputIndex + i] = mul(_Transformations[i], float4(seed, 1)).xyz;
    }
}